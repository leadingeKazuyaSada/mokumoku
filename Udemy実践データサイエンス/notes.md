# 2.

## データの種類

- 数値
    - 連続的
    - 離散的
- カテゴリ
(順序や値の大きさに意味がない)
- 順序
(値の大きさに意味がない)

## 母分散と不偏標本分散

- 標本データを見ている時は、母分散の代わりに「不偏標本分散」を使う。
    - サンプル数 N の場合、N - 1 で割る。
    $\frac{1}{(N - 1)}\sum_{i=1}^{N}(x_i - \bar{X})^2$
    - 一般的に標本の分散は母集団の分散より小さくなるから、それを打ち消す。

## 確率密度関数

- ある一つのデータの値が特定の範囲に収まる確率を表現できる。
- 確率**密度**関数は**連続的**な値をとる。
- 確率**質量**関数は**離散的**な値をとる。

## パーセンタイル

- 50パーセンタイル: データを並べて、下から50%の位置にあるデータの値。
- numpy の `percentile` 関数で求められる。

## モーメント

確率密度関数の形 (の特徴) を表す値

$$
\mu_n = \int_{-\infty}^{\infty}(x - c)^nf(x)dx \hspace{10pt} (値 c の周りの n 次モーメント)
$$

- 1次のモーメント: 平均 (np.mean())
- 2次のモーメント: 分散 (np.var())
- 3次のモーメント: 歪度 (わいど) (scipy.stats.skew())
    - 分布がどれだけ偏っているか
    - グラフが左に伸びていれば負、右に伸びていれば正
- 4次のモーメント: 尖度 (せんど) (scipy.stats.kurtosis())
    - ピークがどれだけ尖っているか
    - 両脇がどれだけ広がっているか

## 相関

相関係数: 共分散をそれぞれのデータセットの標準偏差で割ったもの (-1～1 となる)。

$$相関係数 = \frac{X, Y の共分散}{(X の標準偏差)(Y の標準偏差)}$$

- `np.corrcoef()` で相関係数を求められる。
- `np.cov()` で共分散を求められる。
- `scipy.stats.linregress()` で、相関係数を含む各種係数を求められる。

相関は、因果関係を示している訳ではない。

## 条件付き確率

A が起きた時に (それが確定した上で) B が起きる確率は、A と B 両方が起きる確率を、A が起きる確率で割ったもの。

$$P(B|A) = \frac{P(A, B)}{P(A)}$$

# 3. 予測モデル

## 線形回帰

データにフィットする直線を求める。
(データが直線上に並んでいると仮定する。)

求めるには、以下の方法がある。
- 最小二乗法
    - 二乗誤差の和を最小化する (最尤推定)。
- 最急降下法
    - 何度も線を引く。

`scipy.stats.linregress()` で直線を求められる。

モデルの適切性 (その直線が良い予測式になっているか)
- R-二乗値 (決定係数) による測定
    - 偏差の二乗和に対する誤差の二乗和の割合
    $$1.0 - \frac{(直線からの) 誤差の二乗和}{(平均からの) 偏差の二乗和}$$
    - 0-1 の範囲
    - 0 -> その直線は予測に使えない
    - 1 -> その直線は予測に使える

直線からの誤差が0に近づく -> R-二乗値が1に近づく。

相関係数を二乗するとR-二乗値になる (らしい)。

## 多項式回帰

直線ではなく、曲線でデータを近似する。
(そしてそれを予測に使う。)

可視化することで、以下のことに気をつける
- 必要以上の次数を使っていないか
- 外れ値に適合 (過剰適合) していないか

`np.poly1d(np.polyfit(x, y, 次数))` で曲線の関数を求められる (polyfit は係数を求める)。
`sklearn.metrics.r2_score(実測値の配列, 曲線上の値の配列)` で　R-二乗値を求められる。

- 訓練データとテストデータで R-二乗値を比較することでも、過剰適合をチェックできる。

## 多変量回帰 (重回帰)

- 複数の変数が、ひとつの変数に影響を与える。
- 各変数にそれぞれ係数をつけて線形結合した式が予測式。
- 係数が意味を持つためには、変数は正規化されていること。
- 各パラメタが独立の場合、フィットの具合は R-二乗値で測定できる。
- `statsmodels` の `OLS` で最小二乗法のモデルを作れる。モデルの `summary()` で、定数とパラメタごとの係数、標準誤差を見れる。

# 4. 教師あり学習・教師なし学習

## 単純ベイズ (ナイーブベイズ)

- `sklearn` の `CountVectorizer` を使って、テキストを単語ごとの出現数を表すベクトルに変換できる。
- `sklearn` の `MultinomialNB` を使って、単純ベイズによる分類器を作ることができる。

## エントロピー

- データセットの乱雑さの尺度
- データセットに n 種類のデータがあり、各分類の割合が $p_1$ ～ $p_n$ の時、以下で表される。

$$H(S) = -p_1\ln p_1 - p_2\ln p_2 - ... - p_n\ln p_n$$

(メモ) ln は底が e のことだが、エントロピーについて調べると、底を2とするのが普通みたい。

## 決定木

- 各ステップで、データのエントロピーを最小にできるような分割をする (ID3 というアルゴリズム)。
- pydotplus でツリーを可視化。
- ランダムフォレストをするには、`sklearn.ensemble.RandomForestClassifier`

## アンサンブル学習

- バギング
    - データをランダムに取り出して複数のモデルを作る。
- ブースティング
    - 誤分類されたデータの重みを大きくしつつモデルを何回も作る。
- Bucket of models
    - 複数のモデルを訓練して最も成績の良いものを採用する。
- スタッキング
    - 複数のモデルを同時に適用し、結果を結合する。

## サポートベクターマシン

- 高次元の (特徴をたくさん持つ) データの分類に使える。
- Support Vector Classification という手法。
- データセットにより有効なカーネルが異なる。
    - Linear kernel
    - RBF kernel
    - Polynomial kernel

scikit-learn を使う
- `sklearn.svm.SVC()` で SVC のオブジェクト生成。
- `kernel='linear'` などと指定する。

描画
- `np.meshgrid()`, `plt.contourf()` で等高線を描画できる。

# 5. レコメンドシステム

## ユーザーベース協調フィルタリング

- ユーザごとにパラメタをもたせて、類似度を測れるようにする。
- 似たユーザが評価した商品を勧める。

問題点
- DB 上のパラメタと、ユーザの今の好みが変わっているかも知れない。
- うっかり間違った評価をしているかも知れない。
- ユーザ数が多いので計算量が膨大になる。

## アイテムベース協調フィルタリング

- 類似度を測りたい商品の両方を評価したユーザを抽出する。
- 各ユーザの評価スコアの差から、商品同士の類似度を求める。

前処理
- `DataFrame.pivot_table()` で、表を変形できる。
- `DataFrame.corrwith()` で、2つの DF の同じ列同士の相関係数を求められる (インデックスが揃っていること)。DF と Series を与えると、DF 側の全列に対して Series との相関係数を求める。

類似度1.0の映画がたくさんあった。
- 「スターウォーズと何らかのマイナーな映画だけを評価した人」の票がたくさんあるため。
- マイナーな映画 (少数の人々によって評価された映画) を削除する。
- `DataFrame.groupby().agg()` で集計結果 (元の `rating` 列の `size` と `mean`) の DataFrame を得る。
- `rating` の `size` が100以上の映画だけを抽出した DF を作る。
- それを相関係数の DF と `join()` することで、同じインデックスの行 (`size` が100以上の行) だけを残しつつ、類似度が入った DF を作る。

スターウォーズだけでなく全映画同士の相関係数を求める
- `DF.corr()` で、全映画同士の相関係数の DF を作れる。
- `corr()` のパラメタ `min_periods` で、最低限必要な要素数を指定し、評価した人の数が少ない映画を無視する。

あるユーザにレコメンドをする
- そのユーザが行った評価の一覧を抽出する。
`userRatings.loc[0].dropna()`
- `DF.corr()` で求めた相関係数の DF から、ユーザが評価を行った映画の列を取り出す。
- 全映画 x ユーザが評価した映画について、類似度 x ユーザの評価を、おすすめ度とする。
- 同じ映画に複数のおすすめ度が出るので、足し合わせる。

# 6. その他の技術

## K近傍法

- 距離が近い K 個のデータを取り出し、多数決で予測する。
- 適切な K を設定することが重要 (K によって結果が変わる)。

### 映画データの評価の予測

- 距離を測れるよう、データを正規化する。
- `scipy.spatial.distance.cosine()` で、コサイン距離を求められる。

## 次元の呪い

- データ分析における多くの問題が「次元の呪い」(次元の持ちすぎ) によるもの。
- 次元が高いと可視化もできない。
- 次元削減が (可能なら) 有効。
- 実は K 平均法は次元削減の一種。

### 主成分分析

- `sklearn.decomposition.PCA().fit()` で、次元削減のための変換をするオブジェクトを得る。
- `components_` 属性で、どういう変換をしているか分かる。
- `explained_variance_ratio_` 属性で、変換後の各主成分の因子寄与率 (全体の分散に対する、分散の割合) がわかる。合計が1に近いほど、うまく成分を残している。
- そのオブジェクトを使って `transform()` すると、具体的な変換後の成分 (主成分) の値を得られる。

## データウェアハウス

- ETL
    - Extract (抽出) > Transform (加工) > Load (書き込み)
- ELT
    - 書き込み後に加工を行う。
    - Hadoop, NoSQL など。

## 強化学習

- エージェントが空間を探索し、報酬を多く得られる方へ行く。

### Q 学習

- 条件
    - 状態 s
    - 行動 a
    - 状態・行動と紐付いた値 Q
- 探索のステップごとに、結果によって Q を増減させる
- Q を計算する時に、何ステップか先の結果を使うことができる。
    - ステップ数に応じて割引率を設定する (先に起こることほど、現状の評価への影響を少なくする)。
- 最高の Q だけを求め続けると探索が止まってしまう。
    - $\epsilon$ 確率を用いて、ランダム性を持たせる ($\epsilon$-greedy 法)。

### 関連用語

(メモ) 説明が雑で分からないが、今回は深追いしない。

#### マルコフ決定過程

- 状態遷移が確率的に生じるシステム
- 状態遷移がマルコフ性 (?) を満たす

#### 動的計画法

    - 複数の部分問題に分割する方法

# 7. 実際のデータを使った分析

## バイアス-バリアンスのジレンマ

この教材ではよく分からない。
Web で調べたところ、どうやらモデルに対する指標ではなく、学習アルゴリズムに対する指標のようです。
教師データの選び方にどれくらい引きずられるか、みたいな話。

- [バイアスとバリアンス - アドファイブ日記（ミラー版）](http://chiral.hatenablog.com/entry/20130906/1378438914)

## K分割交差検証における過剰適合の回避

- `sklearn.model_selection.train_test_split()` で、訓練用と検証用のデータに分けられる。
(今は `cross_validation` ではなく `model_selection` を使う)
- `svm.SVC()` でモデルを作る時に `kernel='poly'` にすると、多項式カーネルになる。
- この時、`degree=3` 等とすると、次数を指定できる (3がデフォルト)。
- 次数を変えつつ交差検証すると、過剰適合しないラインを探ることができる。

## データクリーニング

## 数値データの正規化

- `sklearn` の `whiten` で正規化できる。
- 現在はほとんどのデータマイニング/機械学習ライブラリで、正規化せずとも問題なく動作する。

## 外れ値の扱い

- 単に大きい値 (大富豪のデータなど) を除外するのでなく、除外する理由が必要。
- 中央値から、標準偏差2個分より大きく離れた値を、外れ値として除去する方法がある。

# 8. Apache Spark

## インストール

- Cドライブの直下に spark というフォルダを作ってインストールする。
- winutils も作る。

## 簡単なコードを実行

テキストファイルの行数をカウントするコード

```
> pyspark
>>> rdd = sc.textFile("README.md")
>>> rdd.count()
108
```

## Spark の概要

- 大規模はデータ処理のための
- 高速で汎用性の高いエンジン
- スケーラブル
- DAG (directed acyclic graph) がワークフローを最適化
- RDD (Resilient Distributed Dataset) というコンセプト
    - 並列で実行可能なデータセット

Python vs Scala

- コーディングは Python または Scala で行う。
- Spark は Scala で書かれている。
- 新機能やライブラリは Scala が優先されやすい。

Spark Context

- 処理をする際に Spark Context のインスタンスが作られる。
- RDD を生成し、resilient (回復性) と distributed (分散処理) に責任を持つ。

RDD

- RDD は遅延評価する。

## MLLib

MLLib は、Spark に実装されている機械学習のライブラリ

- MLLib 特有のデータ型がある
    - Vector (dense/sparse)
    - LabeledPoint
    - Rating

## Spark における決定木

Spark で .py を実行する
```
(.venv)> spark-submit SparkDecisionTree.py
```

決定木の作り方についての説明は、ソースコードを上からさーっと読んでいく感じだった。  
実行結果が正しいことだけ確認した。

## Spark における K-means

モデルの作り方についての説明は、ソースコードを上からさーっと読んでいく感じだった。

課題をやった結果

- K を 5 -> 10 に増やしたら、結果のリストに 0～9 がまんべんなく出るようになった (5の時は 0～4)。
- 正規化をしないようにしたら WSSSE が 20 前後から 568224 に増えた。
